\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subfig}
\usepackage{float}
\usepackage{microtype}
\usepackage{url}
\usepackage{hyperref}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{NeuroAdaptive Interface (NAI): Real-Time EEG-Based Cognitive State Monitoring Using P300 and Deep Learning}

\author{\IEEEauthorblockN{Anjali Barge}
\IEEEauthorblockA{Computer Engineering Department \\
Alard College of Engineering and Management \\
Pune, India \\
bargeanjali650@gmail.com}
}

\maketitle

\begin{abstract}
This work investigates whether a P300 classifier trained on 19 subjects can generalize to previously unseen individuals. A complete real-time BCI system was developed that streams EEG data, processes it through EEGNet, and displays results on a live dashboard. The primary evaluation employed leave-one-subject-out validation on 20 subjects from the ERP CORE P3b dataset. Cross-subject AUC was 0.57 with a standard deviation of 0.12. While modest at first glance, this performance matches results reported by other groups using strict LOSO protocols. The same model achieved 0.85 to 0.90 AUC when trained and tested on individual subjects. This 30-point performance gap demonstrates that current deep learning models remain heavily dependent on subject-specific patterns. All code is available to reproduce these results.
\end{abstract}

\begin{IEEEkeywords}
EEG, P300, BCI, EEGNet, LOSO, ERP CORE
\end{IEEEkeywords}

\section{Introduction}
A brain--computer interface that works out of the box---no calibration, no training session, simply strap on the headset and begin operation. This vision has driven BCI research for decades \cite{farwell1988talking}. The field has not yet achieved this goal. The hardware is no longer the primary obstacle. Commercial EEG headsets can be purchased for under \$500 and stream data over Bluetooth reliably. The challenge lies in developing classifiers that work across different individuals.

EEG signals are noisy and highly individual. Skull thickness varies. Electrode contact changes. Some people have strong alpha rhythms, others do not. All of these factors affect signal statistics in ways that compromise machine learning models \cite{sutton1965p300,krusienski2006comparison}. A classifier trained on one individual's brain data may fail completely on another person's data---even when both show clear P300 responses to the same task. Most published BCI systems avoid this problem by mixing subjects in their train/test splits, which inflates performance numbers.

This work investigates the severity of the cross-subject generalization problem. A complete real-time BCI pipeline was developed and tested using the rigorous approach of leave-one-subject-out validation on 20 subjects from the ERP CORE P3b dataset. Each fold trains on 19 people and tests on the 20th, with no shortcuts taken.

The classifier uses EEGNet \cite{lawhern2018eegnet}, which learns spatial and temporal filters jointly while keeping parameter count low. Under LOSO, it achieved 0.57 ± 0.12 AUC. This performance matches what other groups report with similarly strict validation. More significantly, the same model achieved 0.85--0.90 AUC when trained and tested within individual subjects. The architecture functions effectively; the cross-subject generalization remains challenging.

This paper makes three contributions. First, it provides honest cross-subject P300 results with full reproducibility. Second, it presents a working real-time system that streams EEG, runs inference, and displays results on a live dashboard. Third, it offers a realistic discussion of the current state of BCI and future research directions.

\section{Related Work}

\subsection{P300 spellers and their calibration burden}

Sutton and colleagues first noticed that unexpected stimuli produce a positive voltage deflection around 300--500~ms post-stimulus \cite{sutton1965p300}. Farwell and Donchin turned this observation into a communication device: flash letters on a screen, detect which flash elicits a P300, and you know which letter the user intended \cite{farwell1988talking}. The idea was elegant. The engineering was not.

Early systems relied on stepwise linear discriminant analysis (SWLDA) and required 15--30 minutes of calibration per user \cite{krusienski2006comparison}. That calibration session collected enough target and non-target epochs to fit a subject-specific classifier. Without it, accuracy dropped sharply. Some groups tried pooling data across subjects, but the resulting models rarely matched subject-specific ones. The P300 response, it turned out, is not as stereotyped as textbooks suggest. Amplitude can vary two- or threefold between individuals; latency can shift by 100~ms or more; scalp topography depends on cortical folding that differs from brain to brain.

\subsection{Compact neural networks for EEG}

Hand-crafted features gave way to learned features once convolutional networks proved they could extract useful representations from raw or minimally filtered EEG. The challenge was parameter count: EEG datasets are small by deep learning standards, and overfitting is easy.

Lawhern et al.\ addressed this with EEGNet \cite{lawhern2018eegnet}, a network built around depthwise and separable convolutions. The first layer learns temporal filters; the second learns spatial filters that mix channels; the third refines the representation. Total parameter count stays in the tens of thousands rather than millions. On motor imagery, error-related potentials, and P300 tasks, EEGNet matched or beat heavier architectures while training faster and generalizing better to held-out trials from the same subject.

The catch is that ``held-out trials from the same subject'' is not the same as ``held-out subjects.'' When researchers split data at the trial level but keep all subjects in both train and test sets, the model can memorize subject-specific quirks. LOSO validation removes that crutch. Under LOSO, EEGNet and similar networks typically land in the 0.55--0.65 AUC range for P300 tasks---above chance, but far from the 0.85+ numbers seen in within-subject splits.

\subsection{Attempts to close the gap}

Several strategies have been proposed to improve cross-subject transfer. Euclidean alignment shifts each subject's covariance matrix toward a common reference. Riemannian methods operate on the manifold of symmetric positive definite matrices, where geodesic distances better capture EEG geometry. Adversarial domain adaptation tries to learn features that a discriminator cannot attribute to any particular subject.

These methods help, but not by much---typically 5--10 percentage points in AUC when no target-subject data is available. Gains jump higher if you allow even a few minutes of calibration from the target user, but that reintroduces the burden the field is trying to escape. The honest summary is that calibration-free cross-subject ERP decoding remains unsolved. Published improvements should be read carefully: many assume access to unlabeled target data, or use validation splits that mix subjects. This work adopts strict LOSO precisely to avoid those pitfalls.

\section{Methods}

\subsection{System overview}
The NAI pipeline (Fig.~\ref{fig:architecture}) includes:
\begin{enumerate}
  \item EEG acquisition using Lab Streaming Layer (LSL) \cite{kothe2019lsl},
  \item preprocessing and epoching (ERP pipeline),
  \item EEGNet inference for P300 detection,
  \item a lightweight cognitive-state mapping and a Streamlit dashboard for real-time feedback.
\end{enumerate}

\begin{figure}[htbp]
  \centerline{\includegraphics[width=0.85\columnwidth]{figures/system_architecture.png}}
  \caption{Conceptual overview of the NAI pipeline showing acquisition, preprocessing, inference, and dashboard components.}
  \label{fig:architecture}
\end{figure}

\subsection{Dataset}
The ERP CORE P3b dataset \cite{kappenman2021erp} was used, hosted on OpenNeuro under accession ds003061. The dataset follows BIDS conventions, which simplified preprocessing procedures. Each of the 20 subjects completed a visual oddball task. Rare stimuli (targets) appeared roughly 80 times per session; frequent stimuli (non-targets) appeared about 320 times. This 1:4 ratio creates the class imbalance addressed in the methodology.

\subsection{Preprocessing}
Preprocessing relied on MNE-Python \cite{gramfort2013mne} (version 1.6.0). The data was band-passed from 0.1 to 30~Hz, epochs were extracted from 0 to 600~ms relative to stimulus onset, baseline correction was applied using the 0 to 100~ms window, trials exceeding a 250~$\mu$V peak-to-peak threshold were rejected, and the data was re-referenced to the common average. This baseline choice empirically reduced early noise variability across subjects and stabilized initial ERP amplitudes under cross-subject evaluation. From these cleaned epochs, grand-average waveforms and topographic maps were computed at the P3b peak.

\subsection{Model and training}
EEGNet \cite{lawhern2018eegnet} was implemented in PyTorch 2.0 with this architecture:
\begin{itemize}
  \item Temporal convolution: $F_1=16$ filters, kernel size 64 samples (around 62 ms at 1024 Hz),
  \item Depthwise spatial convolution: depth multiplier $D=2$, resulting in 32 feature maps,
  \item Separable convolution: $F_2=32$ filters, kernel size 16,
  \item Pooling: average pooling with factors 4 and 8,
  \item Regularization: dropout at $p=0.5$, batch normalization after each convolution,
  \item Classifier: two fully connected layers (128 units, ELU activation).
\end{itemize}

Class imbalance (4:1 non-target to target) required countermeasures. Focal loss ($\gamma=2.0$) with per-fold class weights was used, targets were oversampled during training, and data was augmented on the fly with temporal jitter ($\pm8$~ms), Gaussian noise ($\sigma=0.005$), channel dropout (12\%), and time masking (20~ms blocks). Optimization used Adam (lr=$10^{-3}$, weight decay=$10^{-4}$) with early stopping (patience 8) and learning-rate reduction on plateau.

For LOSO, 20 folds were executed. In each fold, 19 subjects contributed training data (roughly 3768 epochs pooled) and the remaining subject served as the test set (around 200 epochs). Standardization---zero mean, unit variance---was computed on training data only and then applied to the held-out subject. All experiments were run on a laptop (13th Gen Intel i5-13420H, 16~GB RAM, no GPU) with a fixed seed of 42. Full parameter tables appear in Appendix~A.

\subsection{Evaluation metrics}
AUC (area under the ROC curve) is reported as the primary metric because accuracy is misleading when classes are imbalanced 4:1. Mean and standard deviation are computed across the 20 LOSO folds. Scripts to regenerate these numbers are available in the repository.

\section{Results}

\subsection{Neurophysiology}
Before examining classification performance, the presence of a P300 response in the data was verified. Fig.~\ref{fig:erp_topo}(a) plots the grand-average waveform at electrode Pz, where targets show a clear positive deflection peaking near 400~ms. Fig.~\ref{fig:erp_topo}(b) maps scalp voltage at that latency; the parietal maximum confirms the presence of a textbook P3b. Note that the topographic map represents the target minus non-target voltage difference at the P3b peak latency, rather than raw P300 amplitude.

\begin{figure}[htbp]
  \centering
  \subfloat[Grand-average ERP at Pz]{\includegraphics[width=0.48\columnwidth]{figures/erp_grand_target_vs_nontarget.png}}
  \hfill
  \subfloat[Topographic map at P300 peak]{\includegraphics[width=0.48\columnwidth]{figures/topomap_group_peak.png}}
  \caption{ERP and topography illustrating the P3b response derived from ERP CORE P3b data after preprocessing.}
  \label{fig:erp_topo}
\end{figure}

\subsection{Classification performance}
Table~\ref{tab:auc} compares methods. EEGNet reached a LOSO AUC of $0.57 \pm 0.12$, beating LDA ($0.53 \pm 0.10$), SVM ($0.51 \pm 0.09$), and XGBoost ($0.51 \pm 0.11$). When trained and tested within individual subjects, AUC jumped to $0.85$--$0.90$.

\begin{table}[htbp]
\centering
\caption{Classification comparison (LOSO cross-validation, 20 folds). Accuracy is reported for completeness but is misleading due to class imbalance (see text).}
\label{tab:auc}
\begin{tabular}{lccc}
\toprule
Method & AUC (mean $\pm$ std) & Accuracy & Setting \\
\midrule
EEGNet & $0.57 \pm 0.12$ & 0.22 & cross-subject \\
LDA & $0.53 \pm 0.10$ & 0.95 & cross-subject \\
SVM (RBF) & $0.51 \pm 0.09$ & 0.95 & cross-subject \\
XGBoost & $0.51 \pm 0.11$ & 0.95 & cross-subject \\
Logistic Reg. & $0.49 \pm 0.10$ & 0.72 & cross-subject \\
\midrule
EEGNet & $0.85$--$0.90$ & 0.80--0.85 & within-subject \\
\bottomrule
\end{tabular}
\end{table}

The 95\% accuracy for LDA and SVM appears impressive until the underlying mechanism is examined: these models predict "non-target" for almost every trial. Since 80\% of trials are actually non-targets, this strategy yields high accuracy. AUC detects this behavior; accuracy does not. This demonstrates why accuracy numbers are misleading in this context.

\subsection{Per-subject variability}
Performance varied dramatically across subjects. AUC ranged from 0.24 (sub-004, essentially random) to 0.71 (sub-019). Four subjects performed well (AUC > 0.67), but five performed at or below chance level. Analysis of individual ERP waveforms revealed a clear pattern: subjects with weak or delayed P3b peaks were those the model could not classify effectively. The cross-subject model learns from "typical" P300 responses and cannot handle outliers.

Figure~\ref{fig:roc} shows the LOSO ROC curve.

\begin{figure}[htbp]
  \centerline{\includegraphics[width=0.6\columnwidth]{figures/roc_eegnet_loso.png}}
  \caption{ROC curve for EEGNet under LOSO cross-validation (mean AUC = 0.57 across 20 folds).}
  \label{fig:roc}
\end{figure}

\section{Discussion}

\subsection{What the numbers mean}
An AUC of 0.57 appears disappointing until compared with other LOSO studies on P300 data. Most groups report 0.55--0.65 in this setting, placing these results within the expected range. EEGNet performed better than classical methods like LDA and SVM, likely because it learns spatial and temporal patterns jointly instead of using hand-crafted features. The within-subject results (0.85--0.90 AUC) demonstrate that the model architecture is sound---cross-subject generalization remains the primary challenge.

\subsection{Why cross-subject fails}
P300 responses are more variable than textbooks suggest. Amplitude can differ by 2-3x between individuals due to skull thickness and cortical anatomy. Peak timing varies from 300-500 ms, so fixed temporal filters miss some subjects entirely. Head shape affects scalp topography. Electrode impedance, muscle artifacts, and background alpha rhythms create different noise floors for each person. All of these factors compound to make cross-subject learning extremely difficult.

\subsection{Limitations}
Several limitations apply to this work. The ERP CORE dataset consists of clean laboratory data---real-world recordings contain movement artifacts and attention lapses that would degrade performance further. Domain adaptation techniques and transfer learning approaches were not explored. The 0-600 ms analysis window may not be optimal for every subject. Hyperparameters were fixed across all folds instead of being tuned per subject.

\subsection{Future directions}
Several research directions appear promising. Riemannian alignment could help address covariance differences between subjects. Few-shot learning with just 1-2 minutes of target data might bridge the generalization gap. Attention mechanisms could learn to focus on subject-invariant features. Multi-task learning across different ERP components (P3a, P3b, N200) might identify more generalizable patterns.

\subsection{Practical implications}
For clinical applications, subject-specific calibration is still necessary for reliable P300 detection. But a pre-trained cross-subject model could reduce calibration time from 15 minutes to 2-3 minutes with some fine-tuning. The real-time system also enables cognitive monitoring applications where trend detection matters more than perfect accuracy.

\section{Conclusion}
The NeuroAdaptive Interface demonstrates end-to-end real-time P300 detection with honest cross-subject evaluation. The 0.57 ± 0.12 LOSO AUC is modest but realistic. The 30-point gap between cross-subject and within-subject performance shows where the real challenge lies. Future work needs to focus on closing this generalization gap rather than inflating numbers with easier evaluation protocols.

\section*{Acknowledgment}
The author thanks Prof. Anoop Kushwaha (HOD, Computer Engineering), Prof. Manali S. Patil, and Prof. Vrushali More (Alard College of Engineering \& Management) for their mentorship and support during this project. The ERP CORE dataset (OpenNeuro ds003061) was used, with full credit given to the dataset authors.

All code, preprocessing scripts, and publication-quality figures are available in the project repository: \url{https://github.com/AB2511/NAI-project}. The ERP CORE dataset can be found on OpenNeuro (ds003061) under its original license and must be downloaded separately by readers.

The author declares no competing interests relevant to this manuscript.

\appendix
\section{Preprocessing and Training Parameters}
These parameters reflect the exact configuration used to generate all reported figures and results.

\subsection{Preprocessing pipeline}
\begin{itemize}
  \item Raw data format: EEGLAB .set files (BIDS-compliant)
  \item Sampling rate: 1024~Hz (native)
  \item Channels retained: 26 (dropped: FP1, FP2, F7, F8, VEOG, HEOG)
  \item Band-pass filter: 0.1--30~Hz (FIR, firwin design)
  \item Reference: average reference
  \item Epoch window: 0--600~ms post-stimulus
  \item Baseline correction: 0--100~ms
  \item Artifact rejection: 250~$\mu$V peak-to-peak threshold
  \item Event codes: 13 = target (rare), 11/12/14/15 = standard (frequent)
\end{itemize}

\subsection{EEGNet hyperparameters}
\begin{itemize}
  \item Input shape: (26 channels, 615 samples)
  \item $F_1$ (temporal filters): 16
  \item $D$ (depth multiplier): 2
  \item $F_2$ (separable filters): 32
  \item Temporal kernel: 64 samples
  \item Separable kernel: 16 samples
  \item Pooling: AvgPool (4, 8)
  \item Dropout: 0.5
  \item FC layers: 128 $\rightarrow$ 2 (with ELU)
\end{itemize}

\subsection{Training configuration}
\begin{itemize}
  \item Optimizer: Adam (lr=$10^{-3}$, weight decay=$10^{-4}$)
  \item Loss: Focal loss ($\gamma=2.0$, $\alpha$ computed per-fold)
  \item Batch size: 64
  \item Max epochs: 80
  \item Early stopping: patience=8 (on validation AUC)
  \item LR scheduler: ReduceLROnPlateau (factor=0.5, patience=4)
  \item Random seed: 42
\end{itemize}

\section{Per-Subject LOSO Results}
Table~\ref{tab:subject_auc} lists per-subject AUC for the LOSO cross-validation, showing significant variability between subjects.

\begin{table}[htbp]
\centering
\caption{Per-subject AUC results (EEGNet LOSO).}
\label{tab:subject_auc}
\begin{tabular}{cccccc}
\toprule
Subject & AUC & Subject & AUC & Subject & AUC \\
\midrule
sub-001 & 0.61 & sub-008 & 0.63 & sub-015 & 0.46 \\
sub-002 & 0.38 & sub-009 & 0.45 & sub-016 & 0.65 \\
sub-003 & 0.64 & sub-010 & 0.64 & sub-017 & 0.61 \\
sub-004 & 0.24 & sub-011 & 0.67 & sub-018 & 0.47 \\
sub-005 & 0.68 & sub-012 & 0.57 & sub-019 & 0.71 \\
sub-006 & 0.65 & sub-013 & 0.59 & sub-020 & 0.60 \\
sub-007 & 0.68 & sub-014 & 0.52 & & \\
\bottomrule
\end{tabular}
\end{table}

\begin{thebibliography}{00}
\bibitem{sutton1965p300} S. Sutton, M. Braren, J. Zubin, and E. R. John, ``Evoked-potential correlates of stimulus uncertainty,'' Science, vol. 150, no. 3700, pp. 1187--1188, 1965.

\bibitem{farwell1988talking} L. A. Farwell and E. Donchin, ``Talking off the top of your head: toward a mental prosthesis utilizing event-related brain potentials,'' Electroencephalography and Clinical Neurophysiology, vol. 70, no. 6, pp. 510--523, 1988.

\bibitem{krusienski2006comparison} D. J. Krusienski, E. W. Sellers, F. Cabestaing, S. Bayoudh, D. J. McFarland, T. M. Vaughan, and J. R. Wolpaw, ``A comparison of classification techniques for the P300 Speller,'' Journal of Neural Engineering, vol. 3, no. 4, pp. 299--305, 2006.

\bibitem{lawhern2018eegnet} V. J. Lawhern, A. J. Solon, N. R. Waytowich, S. M. Gordon, C. P. Hung, and B. J. Lance, ``EEGNet: a compact convolutional neural network for EEG-based brain--computer interfaces,'' Journal of Neural Engineering, vol. 15, no. 5, p. 056013, 2018.

\bibitem{kothe2019lsl} C. Kothe, ``Lab Streaming Layer (LSL),'' 2019. [Online]. Available: https://github.com/sccn/labstreaminglayer

\bibitem{gramfort2013mne} A. Gramfort, M. Luessi, E. Larson, D. A. Engemann, D. Strohmeier, C. Brodbeck, R. Goj, M. Jas, T. Brooks, L. Parkkonen, and M. Hämäläinen, ``MEG and EEG data analysis with MNE-Python,'' Frontiers in Neuroscience, vol. 7, p. 267, 2013.

\bibitem{kappenman2021erp} E. S. Kappenman, J. L. Farrens, W. Zhang, A. X. Stewart, and S. J. Luck, ``ERP CORE: An open resource for human event-related potential research,'' NeuroImage, vol. 225, p. 117465, 2021.

\end{thebibliography}

\end{document}